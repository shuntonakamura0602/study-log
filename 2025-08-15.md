## Positional Encoding(位置エンコーディング)
「Self-Attention は単語の順序を無視してしまうので、順序情報を数値として入力ベクトルに埋め込む方法」

### 1.なぜ必要？
Self-Attention は全単語を並列処理するので、「どの単語が何番目か」という情報が消えてしまう

例： "I eat apples" と "apples eat I" が区別できなくなる

そこで、位置を表す数値ベクトルを各単語の埋め込みに足し合わせる

### 2.Positional Encoding（Transformer論文より）

位置 $pos$ と埋め込み次元インデックス $i$ を用いて、以下のように定義されます。

$$
\mathrm{PE}(pos, 2i) = \sin\left( \frac{pos}{10000^{\frac{2i}{d_{\mathrm{model}}}}} \right)
$$

$$
\mathrm{PE}(pos, 2i+1) = \cos\left( \frac{pos}{10000^{\frac{2i}{d_{\mathrm{model}}}}} \right)
$$

- $pos$ : 単語の位置（0, 1, 2, …）
- $i$ : 埋め込み次元のインデックス（0, 1, 2, …）
- $d_{\mathrm{model}}$ : 埋め込みベクトルの次元数（例：512）

### 3.直感的な意味

各次元で異なる「波長」のサイン・コサイン波を使う

小さい 𝑖  
i → 波長が短い（細かい変化を捉える）  
大きい 𝑖  
i → 波長が長い（大まかな位置情報を捉える）
サインとコサインを交互に使うことで、位置のシフトに対して線形で計算可能(相対位置も表せる)

### 4.なぜサイン・コサイン？

周期性があるため、位置の相対関係を数値の差で表せる

未知の長さの文でも外挿可能（学習済み位置より長くても関数で計算できる）

モデルが相対距離を簡単に計算できる（内積で距離や位相差を捉えられる）  

### 5.Transformer における使い方

単語の埋め込みベクトル（Word Embedding）を作る

同じ次元の Positional Encoding を計算

それらを単純に足し合わせる

InputEmbedding=WordEmbedding+PositionalEncoding

これをエンコーダ/デコーダの最初の層に入れる

## Positional Encoding が「相対位置」を表せる理由

### 1. そもそも相対位置とは？
Transformer の Attention は、単語同士の「どれくらい離れているか」を使いたいときがあります。  
例えば `"The cat sat on the mat"` なら、`cat` と `mat` は4語離れている、という距離情報がほしい。

相対位置表現ができると：
- **文長が変わっても**距離の計算方法が同じ
- 新しい長さの文章にも外挿可能

---

### 2. サイン・コサインで位置を符号化すると何が起きるか
Positional Encoding の定義（偶数・奇数次元でサインとコサイン）：

$$
\mathrm{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\mathrm{model}}}}}\right)
$$

$$
\mathrm{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\mathrm{model}}}}}\right)
$$

- $pos$ を変えると、**波の位相**が変化
- 次元 $i$ が変わると、波長が変化（短波〜長波まで混ざる）

---

### 3. 相対位置が線形変換で表せる理由
三角関数の加法定理：

$$
\sin(a+b) = \sin a \cos b + \cos a \sin b
$$

$$
\cos(a+b) = \cos a \cos b - \sin a \sin b
$$

位置 $p_2$ を $p_1 + k$ と書くと、  
$k$ は距離（相対位置）にあたるため、加法定理によって  
**「相対位置だけで符号化ベクトルを線形に計算できる」** ことになる。

つまり：
- 位置ベクトル同士の内積や線形結合だけで距離が表せる
- モデルが **相対距離を直接学習** できる

---
